{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType,StructField,StructType,IntegerType,DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 10:57:31 WARN Utils: Your hostname, ubuntu-Lenovo-Legion-5-15ARH05 resolves to a loopback address: 127.0.1.1; using 172.16.5.112 instead (on interface wlp4s0)\n",
      "23/09/12 10:57:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/12 10:57:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Day_6\") \\\n",
    "    .config(\"spark.jars\", \"/usr/lib/jvm/java-11-openjdk-amd64/lib/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tables from postgres  to df\n",
    "listings_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5432/spark_project\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"listing_table\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"postgres\").load()\n",
    "\n",
    "calendar_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5432/spark_project\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"calendar_table\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"postgres\").load()\n",
    "\n",
    "reviews_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5432/spark_project\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"reviews_table\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"postgres\").load()\n",
    "\n",
    "# listing_df_raw.write.jdbc(url=jdbc_url, table='listing_table', mode=\"append\", properties=properties)\n",
    "# calendar_df_raw.write.jdbc(url=jdbc_url, table='calendar_table', mode=\"append\", properties=properties)\n",
    "# reviews_df_raw.write.jdbc(url=jdbc_url, table='reviews_table', mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1) Property Price Categories and Value for Money:**\n",
    "\n",
    "Objective: Divide properties into cheap, mid, and luxury categories based on prices, analyze total bedrooms and bathrooms, and find value-for-money properties along with its sentiment analysis ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|listing_id|      id|sentiment|\n",
      "+----------+--------+---------+\n",
      "|   9896713|88911455| positive|\n",
      "|   9896713|89088569| positive|\n",
      "|   9896713|89654618| positive|\n",
      "|   9896713|89887887| positive|\n",
      "|   9896713|91269769| positive|\n",
      "|   9896713|91839722| positive|\n",
      "|   9896713|92492337|  neutral|\n",
      "|   9896713|93045940| positive|\n",
      "|   9896713|93966900| positive|\n",
      "|   9896713|95209788| positive|\n",
      "|   9896713|95481263| positive|\n",
      "|   9896713|95654848| positive|\n",
      "|   9896713|96811925| positive|\n",
      "|   9896713|97851184| positive|\n",
      "|   9896713|98411027| positive|\n",
      "|   9896713|98871421| positive|\n",
      "|   9896713|99171233| positive|\n",
      "|   9896713|99673240| positive|\n",
      "|   2701124|18828032| positive|\n",
      "|   2701124|21222340| positive|\n",
      "+----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def analyze_sentiment(comment):\n",
    "    if comment is None or not isinstance(comment, str):\n",
    "        # Handle cases where 'comment' is None or not a string\n",
    "        return \"unknown\"\n",
    "    \n",
    "    analysis = TextBlob(comment)\n",
    "    # Classify sentiment as positive, neutral, or negative based on polarity\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return \"positive\"\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "# Register the UDF\n",
    "sentiment_analysis_udf = f.udf(analyze_sentiment, StringType())\n",
    "\n",
    "reviews_sentiment = reviews_df.withColumn(\"sentiment\", sentiment_analysis_udf(f.col(\"comments\")))\n",
    "\n",
    "# reviews_sentiment.show(50, truncate=False)\n",
    "sentiment_df=reviews_sentiment.select(\"listing_id\",\"id\",\"sentiment\")\n",
    "sentiment_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- bedrooms: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- price_category: string (nullable = false)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- value_for_money: double (nullable = true)\n",
      " |-- rank: integer (nullable = false)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n",
      "\n",
      "Ratings According to Sentimental Analysis for Most Value for Money Properties:\n"
     ]
    }
   ],
   "source": [
    "#calculating quartiles for price column. This results in 3 quartiles Q1,Q2 and Q3.\n",
    "quartiles = listings_df.stat.approxQuantile(\"price\", [0.25, 0.5, 0.75], 0.01)\n",
    "from pyspark.sql.functions import when\n",
    "# quantiles\n",
    "\n",
    "#creating UDF for categorizing listings into Cheap, Mid-range and Luxury according to price and taking quartiles as limit.\n",
    "\n",
    "# def categorize_price(price):\n",
    "#     if price <= quartiles[0]: #first quartile\n",
    "#         return \"cheap\"\n",
    "#     elif price <= quartiles[2]: #second quartile\n",
    "#         return \"mid-range\"\n",
    "#     else:\n",
    "#         return \"luxury\"\n",
    "\n",
    "#Registering UDF to spark\n",
    "# categorize_price_udf = f.udf(categorize_price, StringType())\n",
    "\n",
    "# Create a new column 'price_category' based on the categorization\n",
    "# listings_df_1 = listings_df.withColumn(\"price_category\", categorize_price_udf(f.col(\"price\")))\n",
    "# listings_df_1.show()\n",
    "\n",
    "listings_df_1 = listings_df.withColumn(\n",
    "    \"price_category\",\n",
    "    when(f.col(\"price\") <= quartiles[0], \"cheap\")\n",
    "    .when((f.col(\"price\") > quartiles[0]) & (f.col(\"price\") <= quartiles[1]), \"mid-range\")\n",
    "    .otherwise(\"luxury\")\n",
    ")\n",
    "\n",
    "\n",
    "category_stats = listings_df_1.select(\"id\",\"name\",\"bedrooms\", \"bathrooms\", \"price\",\"price_category\",\"host_name\",\"number_of_reviews\")\n",
    "\n",
    "\n",
    "# Calculate value for money as bedrooms + bathrooms per dollar spent\n",
    "category_stats = category_stats.withColumn(\n",
    "    \"value_for_money\",\n",
    "    (f.col(\"bedrooms\") + f.col(\"bathrooms\")) / f.col(\"price\")\n",
    ")\n",
    "\n",
    "window_spec=Window.partitionBy(f.col(\"price_category\")).orderBy(f.col(\"value_for_money\").desc())\n",
    "most_value_for_money = category_stats.withColumn(\"rank\", f.row_number().over(window_spec)).filter(f.col(\"rank\") <=5 ).select(\"*\")\n",
    "# most_value_for_money.show()\n",
    "\n",
    "\n",
    "\n",
    "most_value_for_money = most_value_for_money.withColumnRenamed(\"id\", \"listing_id\")\n",
    "\n",
    "listings_with_sentiment = most_value_for_money.join(sentiment_df, most_value_for_money[\"listing_id\"] == sentiment_df[\"listing_id\"], how=\"left\")\n",
    "listings_with_sentiment=listings_with_sentiment.drop(sentiment_df[\"listing_id\"])\n",
    "listings_with_sentiment.printSchema()\n",
    "listings_with_sentiment_1 = listings_with_sentiment.groupBy(\"listing_id\").agg(f.collect_list(\"sentiment\").alias(\"review_sentiments\"),\n",
    "                                                                              f.first(\"name\").alias(\"name\"),\n",
    "                                                                              f.first(\"bedrooms\").alias(\"bedrooms\"), \n",
    "                                                                              f.first(\"bathrooms\").alias(\"bathrooms\"),\n",
    "                                                                              f.first(\"price\").alias(\"prince\"),\n",
    "                                                                              f.first(\"price_category\").alias(\"price_category\"),\n",
    "                                                                              f.first(\"rank\").alias(\"rank\"))\n",
    "\n",
    "# value_for_money_listings = listings_with_sentiment.filter( most_value_for_money[\"id\"] == most_value_for_money[\"id\"])\n",
    "\n",
    "# Print ratings according to sentimental analysis for these listings\n",
    "print(\"\\nRatings According to Sentimental Analysis for Most Value for Money Properties:\")\n",
    "listings_with_sentiment_1=listings_with_sentiment_1.orderBy(f.col(\"price_category\"))\n",
    "listings_with_sentiment_1=listings_with_sentiment_1.drop(listings_with_sentiment_1[\"listing_id\"])\n",
    "\n",
    "# listings_with_sentiment_1.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/spark_project\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "listings_with_sentiment_1.write.jdbc(url=jdbc_url, table='question_1_table', mode=\"append\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) Find out which month has the most booking(Use quartile to get threshold values to determine the off peak and the peak time) Use these values to list out the properties in peak time and off peak time. Then calculate the total revenue of each host during peak months and calculate their average response rate as well to find correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+------+\n",
      "|listing_id|      date|available| price|\n",
      "+----------+----------+---------+------+\n",
      "|   7308811|2017-05-08|    false|booked|\n",
      "|   7308811|2017-05-07|    false|booked|\n",
      "|   7308811|2017-05-06|    false|booked|\n",
      "|   7308811|2017-05-05|    false|booked|\n",
      "|   7308811|2017-05-04|    false|booked|\n",
      "|   7308811|2017-05-03|    false|booked|\n",
      "|   7308811|2017-05-02|    false|booked|\n",
      "|   7308811|2017-05-01|    false|booked|\n",
      "|   7308811|2017-04-30|    false|booked|\n",
      "|   7308811|2017-04-29|    false|booked|\n",
      "|   7308811|2017-04-28|    false|booked|\n",
      "|   7308811|2017-04-27|    false|booked|\n",
      "|   7308811|2017-04-26|    false|booked|\n",
      "|   7308811|2017-04-25|    false|booked|\n",
      "|   7308811|2017-04-24|    false|booked|\n",
      "|   7308811|2017-04-23|    false|booked|\n",
      "|   7308811|2017-04-22|    false|booked|\n",
      "|   7308811|2017-04-21|    false|booked|\n",
      "|   7308811|2017-04-20|    false|booked|\n",
      "|   7308811|2017-04-19|    false|booked|\n",
      "+----------+----------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calendar_df = calendar_df.withColumn(\"price\", f.regexp_replace(f.col(\"price\"), \"[^0-9]\", \"\").cast(IntegerType()))\n",
    "calendar_df = calendar_df.withColumn(\"price\", f.when(f.col(\"available\") == False, \"booked\").otherwise(f.col(\"price\")))\n",
    "calendar_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+-----------------+\n",
      "|      host_name|total_revenue|avg_response_rate|\n",
      "+---------------+-------------+-----------------+\n",
      "|          Tyler|      4549500|            100.0|\n",
      "|           Faye|      4044000|            100.0|\n",
      "|          Shawn|     14374100|            100.0|\n",
      "|           Chad|      3856200|            100.0|\n",
      "|       Giuseppe|      2376000|93.33333333333333|\n",
      "|        Shannon|      3820000|            100.0|\n",
      "|         Aubrey|      2190500|              0.0|\n",
      "|         Andree|       894000|            100.0|\n",
      "|        Carolyn|      2190000|            100.0|\n",
      "|         Nicolo|      2550000|            100.0|\n",
      "|         Kashif|      6080000|              0.0|\n",
      "| Emily And Carl|      8425000|             90.0|\n",
      "|            Sue|     13440000|             80.0|\n",
      "|           Rich|        17900|              0.0|\n",
      "|    Maria Elena|       810000|            100.0|\n",
      "|          Scott|     11664100|             91.4|\n",
      "|        Sanchit|      3471100|              0.0|\n",
      "|Paul And Lauren|      2580000|             80.0|\n",
      "|            Len|       200000|             90.0|\n",
      "|             Em|       150000|             90.0|\n",
      "+---------------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate booking counts by month\n",
    "booking_counts_df = calendar_df.groupBy(f.month(\"date\").alias(\"month\")).agg(f.count(\"*\").alias(\"booking_count\")).orderBy(f.col(\"booking_count\"))\n",
    "result_df = booking_counts_df.join(calendar_df, (f.month(calendar_df[\"date\"]) == booking_counts_df[\"month\"]), \"left\")\n",
    "\n",
    "# Calculate quartiles for booking counts\n",
    "quartiles = result_df.approxQuantile(\"booking_count\", [0.25, 0.75], 0.001)\n",
    "q1 = quartiles[0]\n",
    "q3 = quartiles[1]\n",
    "\n",
    "\n",
    "\n",
    "#Create a new column in result_df to categorize months\n",
    "result_df = result_df.withColumn(\"month_category\", (f.col(\"booking_count\") >= q1) & (f.col(\"booking_count\") <= q3))\n",
    "\n",
    "\n",
    "# Join with listings_df based on the id\n",
    "result_df_1 = listings_df.join(result_df, (listings_df[\"id\"]) == result_df[\"listing_id\"], \"left\")\n",
    "result_df_1 = result_df_1.withColumn(\n",
    "    \"month_category\",\n",
    "    f.when(f.col(\"month_category\") == True, \"peak\").otherwise(\"off_peak\")\n",
    ")\n",
    "\n",
    "\n",
    "result_df_1 = result_df_1.withColumn(\"revenue\", f.when(result_df[\"available\"] == \"false\", listings_df[\"price\"]).otherwise(0))\n",
    "# result_df_1.filter(f.col(\"available\") == False).show()\n",
    "result_df_1 = result_df_1.withColumn(\"host_response_rate\", f.regexp_replace(f.col(\"host_response_rate\"), \"%\", \"\").cast(\"int\"))\n",
    "# result_df_1.show()\n",
    "\n",
    "\n",
    "total_revenue_by_host = result_df_1.filter(f.col(\"month_category\") == 'peak').groupBy(\"host_name\").agg(f.sum(\"revenue\").alias(\"total_revenue\"),f.coalesce(f.avg(\"host_response_rate\"),f.lit(0)).alias(\"avg_response_rate\"))\n",
    "# total_revenue_by_host.show()\n",
    "\n",
    "correlation = total_revenue_by_host.corr(\"total_revenue\", \"avg_response_rate\")\n",
    "# correlation\n",
    "\n",
    "\n",
    "total_revenue_by_host.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/spark_project\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "total_revenue_by_host.write.jdbc(url=jdbc_url, table='question_2_table', mode=\"append\", properties=properties)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
