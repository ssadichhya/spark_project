{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, avg,udf, month,max, datediff, lit, to_date,count, rank,regexp_replace\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType,StructField,StructType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/08 17:03:32 WARN Utils: Your hostname, ubuntu-Lenovo-Legion-5-15ARH05 resolves to a loopback address: 127.0.1.1; using 172.16.5.112 instead (on interface wlp4s0)\n",
      "23/09/08 17:03:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/08 17:03:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/08 17:03:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Day_6\") \\\n",
    "    .config(\"spark.jars\", \"/usr/lib/jvm/java-11-openjdk-amd64/lib/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the CSV data into DataFrames\n",
    "listing_df_raw = spark.read.csv('raw_data/listings.tsv', header=True, inferSchema=True, sep=\"\\t\")\n",
    "reviews_df_raw = spark.read.csv('raw_data/reviews.tsv', header=True, inferSchema=True, sep=\"\\t\")\n",
    "calendar_df_raw = spark.read.csv('raw_data/calendar.tsv', header=True, inferSchema=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "\n",
    "# LISTINGS DATA\n",
    "\n",
    "# Drop the 'summary' column\n",
    "listing_df_raw = listing_df_raw.drop('summary')\n",
    "listing_df_raw = listing_df_raw.drop('description')\n",
    "listing_df_raw = listing_df_raw.drop('host_about')\n",
    "\n",
    "\n",
    "# Convert 'host_is_superhost' to boolean\n",
    "listing_df_raw = listing_df_raw.withColumn('host_is_superhost', when(col('host_is_superhost') == 't', True).otherwise(False))\n",
    "\n",
    "# Drop 'country' and 'market' columns\n",
    "listing_df_raw = listing_df_raw.drop('country', 'market')\n",
    "\n",
    "# Drop rows with null values in the 'space' column\n",
    "listing_df_raw = listing_df_raw.na.drop(subset=['space'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace commas in all columns using a loop\n",
    "for column in listing_df_raw.columns:\n",
    "    listing_df_raw = listing_df_raw.withColumn(column, regexp_replace(col(column), ',', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALENDAR DATA\n",
    "\n",
    "# Convert 'available' to boolean\n",
    "calendar_df_raw = calendar_df_raw.withColumn('available', when(col('available') == 't', True).otherwise(False))\n",
    "\n",
    "# Remove \"$\" and convert to integer\n",
    "listing_df_raw = listing_df_raw.withColumn(\"price\", regexp_replace(col(\"price\"), \"[^0-9]\", \"\").cast(IntegerType()))\n",
    "\n",
    "#fill price colum with \"booked\"\n",
    "calendar_df_raw = calendar_df_raw.withColumn(\"price\", when(col(\"available\") == False, \"booked\").otherwise(col(\"price\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save data locally\n",
    "\n",
    "listing_df_raw.coalesce(1).write.parquet('cleaned_data/clean_listing', mode=\"overwrite\",compression=\"snappy\")\n",
    "calendar_df_raw.coalesce(1).write.parquet('cleaned_data/clean_calendar',  mode=\"overwrite\",compression=\"snappy\")\n",
    "reviews_df_raw.coalesce(1).write.parquet('cleaned_data/clean_reviews',  mode=\"overwrite\" ,compression=\"snappy\")\n",
    "\n",
    "# reviews_df_raw.repartition(10).write.parquet('cleaned_data/clean_reviews',  mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/spark_project\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Write the DataFrame to the existing PostgreSQL table\n",
    "listing_df_raw.write.jdbc(url=jdbc_url, table='listing_table', mode=\"append\", properties=properties)\n",
    "calendar_df_raw.write.jdbc(url=jdbc_url, table='calendar_table', mode=\"append\", properties=properties)\n",
    "reviews_df_raw.write.jdbc(url=jdbc_url, table='reviews_table', mode=\"append\", properties=properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
