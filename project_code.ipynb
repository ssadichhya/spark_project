{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import functions as f \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "from dotenv import load_dotenv\n",
    "from textblob import TextBlob\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Day_6\") \\\n",
    "    .config(\"spark.jars\", \"/usr/lib/jvm/java-11-openjdk-amd64/lib/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV data into DataFrames\n",
    "listing_df_raw = spark.read.csv('raw_data/listings.tsv', header=True, inferSchema=True, sep=\"\\t\")\n",
    "reviews_df_raw = spark.read.csv('raw_data/reviews.tsv', header=True, inferSchema=True, sep=\"\\t\")\n",
    "calendar_df_raw = spark.read.csv('raw_data/calendar.tsv', header=True, inferSchema=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace commas in all columns using a loop\n",
    "for column in listing_df_raw.columns:\n",
    "    listing_df_raw = listing_df_raw.withColumn(column, regexp_replace(col(column), ',', ''))\n",
    "\n",
    "# Transform the data\n",
    "\n",
    "# LISTINGS DATA\n",
    "\n",
    "# Drop the 'summary' column\n",
    "listing_df_raw = listing_df_raw.drop('summary')\n",
    "listing_df_raw = listing_df_raw.drop('description')\n",
    "listing_df_raw = listing_df_raw.drop('host_about')\n",
    "\n",
    "\n",
    "# Convert 'host_is_superhost' to boolean\n",
    "listing_df_raw = listing_df_raw.withColumn('host_is_superhost', when(col('host_is_superhost') == 't', True).otherwise(False))\n",
    "\n",
    "# Drop 'country' and 'market' columns\n",
    "listing_df_raw = listing_df_raw.drop('country', 'market')\n",
    "\n",
    "# Drop rows with null values in the 'space' column\n",
    "listing_df_raw = listing_df_raw.na.drop(subset=['space'])\\\n",
    "\n",
    "\n",
    "# Remove \"$\" and convert to integer\n",
    "listing_df_raw = listing_df_raw.withColumn(\"price\", regexp_replace(col(\"price\"), \"[^0-9]\", \"\").cast(IntegerType()))\n",
    "\n",
    "\n",
    "\n",
    "# CALENDAR DATA\n",
    "\n",
    "# Convert 'available' to boolean\n",
    "calendar_df_raw = calendar_df_raw.withColumn('available', when(f.col('available') == 't', True).otherwise(False))\n",
    "\n",
    "#fill price colum with \"booked\"\n",
    "calendar_df_raw = calendar_df_raw.withColumn(\"price\", when(f.col(\"available\") == False, \"booked\").otherwise(f.col(\"price\")))\n",
    "\n",
    "calendar_df_raw = calendar_df_raw.withColumn(\"price\", f.regexp_replace(f.col(\"price\"), \"[^0-9]\", \"\").cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save data locally\n",
    "\n",
    "listing_df_raw.coalesce(1).write.parquet('cleaned_data/clean_listing', mode=\"overwrite\",compression=\"snappy\")\n",
    "calendar_df_raw.coalesce(1).write.parquet('cleaned_data/clean_calendar',  mode=\"overwrite\",compression=\"snappy\")\n",
    "reviews_df_raw.coalesce(1).write.parquet('cleaned_data/clean_reviews',  mode=\"overwrite\" ,compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#load data to postgres\n",
    "\n",
    "# Define the JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/spark_project\"\n",
    "properties = {\n",
    "    \"user\":os.environ.get(\"user\"),\n",
    "    \"password\":os.environ.get(\"password\"),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Write the DataFrame to the existing PostgreSQL table\n",
    "listing_df_raw.write.jdbc(url=jdbc_url, table='listing_table', mode=\"overwrite\", properties=properties)\n",
    "calendar_df_raw.write.jdbc(url=jdbc_url, table='calendar_table', mode=\"overwrite\", properties=properties)\n",
    "reviews_df_raw.write.jdbc(url=jdbc_url, table='reviews_table', mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Data From Postgres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tables from postgres  to df\n",
    "listings_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5432/spark_project\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"listing_table\") \\\n",
    "    .option(\"user\", os.environ.get(\"user\")).option(\"password\", os.environ.get(\"password\")).load()\n",
    "\n",
    "calendar_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5432/spark_project\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"calendar_table\") \\\n",
    "    .option(\"user\", os.environ.get(\"user\")).option(\"password\", os.environ.get(\"password\")).load()\n",
    "\n",
    "reviews_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5432/spark_project\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"reviews_table\") \\\n",
    "    .option(\"user\", os.environ.get(\"user\")).option(\"password\", os.environ.get(\"password\")).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1) Property Price Categories and Value for Money:**\n",
    "\n",
    "Divide properties into cheap, mid, and luxury categories based on prices, analyze total bedrooms and bathrooms, and find value-for-money properties along with its sentiment analysis ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ratings According to Sentimental Analysis for Most Value for Money Properties:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+---------+------+--------------+----+\n",
      "|   review_sentiments|                name|bedrooms|bathrooms|prince|price_category|rank|\n",
      "+--------------------+--------------------+--------+---------+------+--------------+----+\n",
      "|          [positive]|Cleveland Circle ...|       1|      3.0|  4500|         cheap|   4|\n",
      "|[positive, positi...|Cozy room in a ch...|       1|      2.5|  4000|         cheap|   5|\n",
      "|          [positive]|Modern Apartment ...|       1|      2.5|  3500|         cheap|   3|\n",
      "|          [positive]|One year from Sep...|       1|      1.5|  2000|         cheap|   1|\n",
      "|[positive, negati...|Private room in C...|       1|      3.0|  3800|         cheap|   2|\n",
      "|                  []|     Vacation Rental|       4|      2.0| 18400|        luxury|   5|\n",
      "|[positive, positi...|The Grand View | ...|       4|      2.0| 17500|        luxury|   3|\n",
      "|          [positive]|CITY HOME COUNTRY...|       4|      2.5| 19900|        luxury|   4|\n",
      "|[positive, positi...|Full beautiful ho...|       4|      2.0| 15900|        luxury|   2|\n",
      "|          [positive]|4BD/3.5BA Perfect...|       4|      3.5| 17500|        luxury|   1|\n",
      "|[positive, positive]|West Broadway Qua...|       1|      6.0| 14400|     mid-range|   4|\n",
      "|                  []|Quarters on Dot -...|       1|      5.0| 10500|     mid-range|   3|\n",
      "|[positive, positive]|Quarters on Dot -...|       1|      5.0|  9500|     mid-range|   2|\n",
      "|          [negative]|West Broadway Qua...|       1|      6.0| 14400|     mid-range|   5|\n",
      "|[positive, positive]|Quarters on Dot -...|       1|      5.0|  9500|     mid-range|   1|\n",
      "+--------------------+--------------------+--------+---------+------+--------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#UDF for sentimantal analysis\n",
    "\n",
    "def analyze_sentiment(comment):\n",
    "    if comment is None or not isinstance(comment, str):\n",
    "        # Handle cases where 'comment' is None or not a string\n",
    "        return \"unknown\"\n",
    "    \n",
    "    analysis = TextBlob(comment)\n",
    "    # Classify sentiment as positive, neutral, or negative based on polarity\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return \"positive\"\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "# Register the UDF\n",
    "sentiment_analysis_udf = f.udf(analyze_sentiment, StringType())\n",
    "\n",
    "reviews_sentiment = reviews_df.withColumn(\"sentiment\", sentiment_analysis_udf(f.col(\"comments\")))\n",
    "\n",
    "# reviews_sentiment.show(50, truncate=False)\n",
    "sentiment_df=reviews_sentiment.select(\"listing_id\",\"id\",\"sentiment\")\n",
    "\n",
    "\n",
    "#calculating quartiles for price column. This results in 3 quartiles Q1,Q2 and Q3.\n",
    "quartiles = listings_df.stat.approxQuantile(\"price\", [0.25, 0.5, 0.75], 0.01)\n",
    "\n",
    "listings_df_1 = listings_df.withColumn(\n",
    "    \"price_category\",\n",
    "    when(f.col(\"price\") <= quartiles[0], \"cheap\")\n",
    "    .when((f.col(\"price\") > quartiles[0]) & (f.col(\"price\") <= quartiles[1]), \"mid-range\")\n",
    "    .otherwise(\"luxury\")\n",
    ")\n",
    "\n",
    "category_stats = listings_df_1.select(\"id\",\"name\",\"bedrooms\", \"bathrooms\", \"price\",\"price_category\",\"host_name\",\"number_of_reviews\")\n",
    "\n",
    "# Calculate value for money as bedrooms + bathrooms per dollar spent\n",
    "category_stats = category_stats.withColumn(\n",
    "    \"value_for_money\",\n",
    "    (f.col(\"bedrooms\") + f.col(\"bathrooms\")) / f.col(\"price\")\n",
    ")\n",
    "\n",
    "window_spec=Window.partitionBy(f.col(\"price_category\")).orderBy(f.col(\"value_for_money\").desc())\n",
    "most_value_for_money = category_stats.withColumn(\"rank\", f.row_number().over(window_spec)).filter(f.col(\"rank\") <=5 ).select(\"*\")\n",
    "# most_value_for_money.show()\n",
    "\n",
    "most_value_for_money = most_value_for_money.withColumnRenamed(\"id\", \"listing_id\")\n",
    "\n",
    "listings_with_sentiment = most_value_for_money.join(sentiment_df, most_value_for_money[\"listing_id\"] == sentiment_df[\"listing_id\"], how=\"left\")\n",
    "listings_with_sentiment=listings_with_sentiment.drop(sentiment_df[\"listing_id\"])\n",
    "# listings_with_sentiment.printSchema()\n",
    "listings_with_sentiment_1 = listings_with_sentiment.groupBy(\"listing_id\").agg(f.collect_list(\"sentiment\").alias(\"review_sentiments\"),\n",
    "                                                                              f.first(\"name\").alias(\"name\"),\n",
    "                                                                              f.first(\"bedrooms\").alias(\"bedrooms\"), \n",
    "                                                                              f.first(\"bathrooms\").alias(\"bathrooms\"),\n",
    "                                                                              f.first(\"price\").alias(\"prince\"),\n",
    "                                                                              f.first(\"price_category\").alias(\"price_category\"),\n",
    "                                                                              f.first(\"rank\").alias(\"rank\"))\n",
    "\n",
    "print(\"\\nRatings According to Sentimental Analysis for Most Value for Money Properties:\")\n",
    "listings_with_sentiment_1=listings_with_sentiment_1.orderBy(f.col(\"price_category\"))\n",
    "listings_with_sentiment_1=listings_with_sentiment_1.drop(listings_with_sentiment_1[\"listing_id\"])\n",
    "\n",
    "listings_with_sentiment_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load question_1 output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/spark_project\"\n",
    "properties = {\n",
    "    \"user\": os.environ.get(\"user\"),\n",
    "    \"password\":os.environ.get(\"password\"),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "listings_with_sentiment_1.write.jdbc(url=jdbc_url, table='question_1_table', mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2) Total revenue analysis during peak months** \n",
    "\n",
    "Find out which month has the most booking(Use quartile to get threshold values to determine the off peak and the peak time) Use these values to list out the properties in peak time and off peak time. Then calculate the total revenue of each host during peak months and calculate their average response rate as well to find correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+-----------------+\n",
      "|      host_name|total_revenue|avg_response_rate|\n",
      "+---------------+-------------+-----------------+\n",
      "|          Tyler|      4549500|            100.0|\n",
      "|           Faye|      4044000|            100.0|\n",
      "|           Chad|      3856200|            100.0|\n",
      "|       Giuseppe|      2376000|93.33333333333333|\n",
      "|        Shannon|      3820000|            100.0|\n",
      "|        Carolyn|      2190000|            100.0|\n",
      "|          Shawn|     14374100|            100.0|\n",
      "|         Aubrey|      2190500|              0.0|\n",
      "|         Andree|       894000|            100.0|\n",
      "|         Nicolo|      2550000|            100.0|\n",
      "|         Kashif|      6080000|              0.0|\n",
      "| Emily And Carl|      8425000|             90.0|\n",
      "|            Sue|     13440000|             80.0|\n",
      "|            Len|       200000|             90.0|\n",
      "|             Em|       150000|             90.0|\n",
      "|          Scott|     11664100|             91.4|\n",
      "|           Rich|        17900|              0.0|\n",
      "|    Maria Elena|       810000|            100.0|\n",
      "|        Sanchit|      3471100|              0.0|\n",
      "|Paul And Lauren|      2580000|             80.0|\n",
      "+---------------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "the correlation between total revenue and average response rate: 0.039774165822028314\n"
     ]
    }
   ],
   "source": [
    "# Calculate booking counts by month\n",
    "booking_counts_df = calendar_df.groupBy(f.month(\"date\").alias(\"month\")).agg(f.count(\"*\").alias(\"booking_count\")).orderBy(f.col(\"booking_count\"))\n",
    "result_df = booking_counts_df.join(calendar_df, (f.month(calendar_df[\"date\"]) == booking_counts_df[\"month\"]), \"left\")\n",
    "\n",
    "# Calculate quartiles for booking counts\n",
    "quartiles = result_df.approxQuantile(\"booking_count\", [0.25, 0.75], 0.001)\n",
    "q1 = quartiles[0]\n",
    "q3 = quartiles[1]\n",
    "\n",
    "\n",
    "#Create a new column in result_df to categorize months\n",
    "result_df = result_df.withColumn(\"month_category\", (f.col(\"booking_count\") >= q1) & (f.col(\"booking_count\") <= q3))\n",
    "\n",
    "\n",
    "# Join with listings_df based on the id\n",
    "result_df_1 = listings_df.join(result_df, (listings_df[\"id\"]) == result_df[\"listing_id\"], \"left\")\n",
    "result_df_1 = result_df_1.withColumn(\n",
    "    \"month_category\",\n",
    "    f.when(f.col(\"month_category\") == True, \"peak\").otherwise(\"off_peak\")\n",
    ")\n",
    "\n",
    "\n",
    "result_df_1 = result_df_1.withColumn(\"revenue\", f.when(result_df[\"available\"] == \"false\", listings_df[\"price\"]).otherwise(0))\n",
    "# result_df_1.filter(f.col(\"available\") == False).show()\n",
    "result_df_1 = result_df_1.withColumn(\"host_response_rate\", f.regexp_replace(f.col(\"host_response_rate\"), \"%\", \"\").cast(\"int\"))\n",
    "# result_df_1.show()\n",
    "\n",
    "\n",
    "total_revenue_by_host = result_df_1.filter(f.col(\"month_category\") == 'peak').groupBy(\"host_name\").agg(f.sum(\"revenue\").alias(\"total_revenue\"),f.coalesce(f.avg(\"host_response_rate\"),f.lit(0)).alias(\"avg_response_rate\"))\n",
    "# total_revenue_by_host.show()\n",
    "\n",
    "correlation = total_revenue_by_host.corr(\"total_revenue\", \"avg_response_rate\")\n",
    "\n",
    "\n",
    "total_revenue_by_host.show()\n",
    "print(\"the correlation between total revenue and average response rate:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load question_2 output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/spark_project\"\n",
    "properties = {\n",
    "    \"user\": os.environ.get(\"user\"),\n",
    "    \"password\": os.environ.get(\"password\"),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "total_revenue_by_host.write.jdbc(url=jdbc_url, table='question_2_table', mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
